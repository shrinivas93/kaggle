# Machine Learning

- Data Preprocessing
	- Importing the libraries
		- numpy
			- Mathematical toolkit
			- Arrays toolkit
		- matplotlib.pyplot
			- Visualization
		- pandas
			- Data IO
			- Data manipulation
	- Importing the data (pandas)
		- Read the data
		- Split data into
			- Matrix of features (X)
			- Array of Prediction (y)
	- Handle missing data
		- Remove record with missing data
		- Replace missing data with column mean (sklearn.preprocessing - Imputer)
		- Replace missing data with column median (sklearn.preprocessing - Imputer)
		- Replace missing data with most frequent value (sklearn.preprocessing - Imputer)
		- Replace missing data with most frequent value in similar type of data
		- Predict the missing value using KNN (categorical data)
		- Treat missing data as another category (categorical data)
	- Handle categorical data
		- Label Encoding (sklearn.preprocessing - LabelEncoder)
			- In case of comparable data
			- Ex: [Low, Medium, High]
		- One Hot Encoding (sklearn.preprocessing - OneHotEncoder)
			- In case of non comparable data
			- Ex: [Male, Female]
			- Creates Dummy Variable with boolean data for each category value of every categorical feature column
			- Remove 1 Dummy Variable to avoid Dummy Variable Trap
	- Split data into Training & Testing dataset (sklearn.cross_validation - train_test_split)
		- Good Test Size (0.2, 0.25, 0.3)
		- Build model on Train Data
		- Verify model on Test Data
		- Calculate accuracy using Predicted values and test values
	- Feature Scaling
		- Necessary to avoid biased predictions
		- Fit scaler over train data and transform both train data and test data
		- Standardization
			- Denotes how many standard deviations away the value is from the mean
		- Normalization
			- Scales the values between 0 and 1
- Regression
	- Linear Regression Assumptions
		- Linearity
		- Homoscedasticity
		- Multivariate normality
		- Independence of errors
		- Lack of multicollinearity 
	- Simple Linear Regression (sklearn.linear_model - LinearRegression)
		- y = b<sub>0</sub> + b<sub>1</sub>\*x<sub>1</sub>
		- Fits a straight line with b1 slope and b0 offset on y axis
		- Squared Sum Error becomes minimum
	- Multiple Linear Regression (sklearn.linear_model - LinearRegression)
		- y = b<sub>0</sub> + b<sub>1</sub>\*x<sub>1</sub> + ... + b<sub>n</sub>\*x<sub>n</sub>
		- Fits a straight line depending upon features (x<sub>1</sub>...x<sub>n</sub>)
		- Squared Sum Error becomes minimum
		- Methods to build model
			- All-in
				- Use all features
				- Lazy approach
				- Not Recommended
			- Backward Elimination (Recommended, Fastest)
				- Select Significance Level (SL) (Around 0.05)
				- Fit the model with all features and remove the feature with highest P-value if P-value > SL
				- Repeat this until no more feature can be removed
			- Forward Selection
				- Select Significance Level (SL) (Around 0.05)
				- Fit all possible model with single feature at a time and add the feature with least P-value
				- Keep adding another feature to the model, fir the model and iteratively add new feature with lowest P-value less than SL
				- Repeat this until no more features can be added
			- Bidirectional Elimination (Stepwise Regression) (Too much computation)
				- Select Significance Level (SL) (Around 0.05)
				- For every pass of `Forward Selection`, run entire `Backward Elimination`
				- Repeat this until no more features can be added or removed
			- Score Comparison
				- Build all possible models (2<sup>N</sup>-1) (N - No. of features)
				- Use the one with best results
				- Exponential, Not scalable with higher no. of features
	- Polynomial Regression
		- y = b<sub>0</sub> + b<sub>1</sub>\*x<sub>1</sub> + b<sub>2</sub>\*x<sub>1</sub><sup>2</sup> + ... + b<sub>n</sub>\*x<sub>1</sub><sup>n</sup>
		- Fits a non-linear curve depending on the degree of equation
		- Expand the feature set by different powers of existing feature
		- Implement `Multiple Linear Regression` on expanded feature sets
		- Higher the degree of polynomial, better are the predictions
	- Support Vector Regression (sklearn.svm - SVR)
		- Fits linear, non-linear, or any curve based on the dataset
		- Works well in case of small dataset
	- Decision Tree Regression (sklearn.tree - DecisionTreeRegression)
		- Non-continuos regression model
		- Splits the dataset space into different zones based on their values and forms a tree
		- Predicts the value of new data by calculating the average of all data points in the selected zone
		- Not very good in case of single feature
	- Random Forest Regression (sklearn.tree - RandomForestRegressor)
		- More than one Decision trees used for prediction
		- A type of `Ensemble Learning` *(multiple instances of same or different techniques used for prediction)*
		- Steps
			- Take random `K` datapoints from training set and fit a decision tree
			- Fit `N` decision trees using above approach
			- For new datapoint, predict value using all decision trees and use its average value
		- Higher number of trees predict better
	- Performance measures for Regression
		- R-Squared
			- R<sup>2</sup> = 1 - SS<sub>res</sub> / SS<sub>tot</sub>  
			  where, SS<sub>res</sub> = SUM(y<sub>i</sub> - y<sub>i</sub>')<sup>2</sup>  
			  and, SS<sub>tot</sub> = SUM(y<sub>i</sub> - y<sub>avg</sub>)<sup>2</sup>
			- Denotes how better is the model compared to average values of training output
			- Higher the value of R<sup>2</sup>, better is the model compared to average values of training output
		- Adjusted R-Squared
			- Comes into picture because R<sup>2</sup> is misleading in case of multiple features
			- If more features are added, R<sup>2</sup> increases even if the model is worsening
			- Adj R<sup>2</sup> = 1 - (1 - R<sup>2</sup>) \* (n - 1) / (n - p - 1)  
			  where n = size of sample data
			  and p = number of independent variables
			- It penalizes for addition of independent variable and also takes into account the little increase in R<sup>2</sup> due to addition of bad independent variable
			- While implementing `Backward Elimination`, consider Adjusted R<sup>2</sup> value also to check whether removing the variable was the smart move
		- Estimate coefficient in regressor.summary()
			- Denotes the correlation between the variable and the output if all other variables are kept same
- Classification
	- Logistic Regression (sklearn.linear_model - LogisticRegression)
		- Predicts the probability of target variable based on the features
		- Can be used as classifier based on the calculation done on the probability
		- Fits a straight line
		- Check accuracy of model using Confusion Matrix (sklearn.metrics - confusion_matrix)
		- It is a linear classifier (Seperates two classes by a straight line, plane or hyper-plane)
	- K - Nearest Neighbour (sklearn.neighbors - KNeighborsClassifier)
		- Classifies test datapoint based on the class of majority of training datapoints out of the K nearest neighbours
		- Calculate distance of test data with all datapoints of training data.
		- Find nearest K datapoints and choose the class of majority datapoints  from neighbours as class for the test data
		- Classifies randomly based on the distance of test datapoint with K nearest neighbours of training data
	